% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/OptHoldoutSize_emulation.R
\name{mu_fn}
\alias{mu_fn}
\alias{powerlaw}
\title{Power law function}
\usage{
powerlaw(n, theta)

mu_fn(
  n,
  nset,
  d,
  var_w,
  N,
  k1,
  var_u = 1e+07,
  k_width = 5000,
  mean_fn = powerlaw,
  theta = powersolve(nset, d, y_var = var_w)$par
)
}
\arguments{
\item{n}{Set of training set sizes to evaluate}

\item{theta}{Current estimates of parameter values for mean_fn. Defaults to the MLE power-law solution corresponding to n,d, and var_w.}

\item{nset}{Training set sizes for which a loss has been evaluated}

\item{d}{Loss at training set sizes \code{nset}}

\item{var_w}{Variance of error in loss estimate at each training set size.}

\item{N}{Total number of samples on which the model will be fitted/used}

\item{k1}{Mean loss per sample with no predictive score in place}

\item{var_u}{Marginal variance for Gaussian process kernel. Defaults to 1e7}

\item{k_width}{Kernel width for Gaussian process kernel. Defaults to 5000}

\item{mean_fn}{Functional form governing expected loss per sample given sample size. Should take two parameters: n (sample size) and theta (parameters). Defaults to function \code{powerlaw}.}
}
\value{
Vector of values of same length as \code{n}


}
\description{
Power law function for modelling learning curve (taken to mean change in expected loss per sample with training set size)

Recommended in \href{https://arxiv.org/abs/2103.10948}{review of learning curve forms}

If \code{theta=c(a,b,c)} then models as \verb{a n^(-b) + c}. Note \code{b} is negated.

Note that \code{powerlaw(n,c(a,b,c))} has limit \code{c} as \code{n} tends to infinity, if \verb{a,b > 0}

Posterior mean for emulator given points \code{n}.
}
\examples{

ncheck=seq(1000,10000)
plot(ncheck, powerlaw(ncheck, c(5e3,1.2,0.3)),type="l",xlab="n",ylab="powerlaw(n)")


# Suppose we have population size and cost-per-sample without a risk score as follows
N=100000
k1=0.4

# Suppose we begin with loss estimates at n-values
nset0=c(10000,20000,30000)

# with cost-per-individual estimates
d0=c(0.35,0.26,0.31)

# and associated error on those estimates
var_w0=c(0.1^2,0.08^2,0.12^2)

# We estimate theta from these three points
theta0=powersolve(nset0,d0,y_var=var_w0)$par

# We will estimate the posterior at these values of n
n=seq(1000,50000,length=1000)

# Mean and variance
p_mu=mu_fn(n,nset=nset0,d=d0,var_w = var_w0, N=N,k1=k1,theta=theta0,k_width=5000,var_u=3000000)
p_var=psi_fn(n,nset=nset0,N=N,var_w = var_w0,k_width=5000,var_u=3000000)
p_var=p_var[cbind(1:length(n),1:length(n))] # just the variances

# Plot
plot(0,xlim=range(n),ylim=range(c(p_mu - 3*sqrt(p_var),p_mu + 3*sqrt(p_var))),type="n",
  xlab="Training/holdout set size",
  ylab="Total cost (= num. cases)")
lines(n,p_mu,col="blue")
lines(n,p_mu - 3*sqrt(p_var),col="red")
lines(n,p_mu + 3*sqrt(p_var),col="red")
points(nset0,k1*nset0 + d0*(N-nset0),pch=16,col="purple")
lines(n,k1*n + powerlaw(n,theta0)*(N-n),lty=2)
legend("topright",
  c(expression(mu(n)),
    expression(mu(n) \%+-\% 3*sqrt(psi(n))),
    "prior(n)",
    "d"),
  lty=c(1,1,2,NA),lwd=c(1,1,1,NA),pch=c(NA,NA,NA,16),pt.cex=c(NA,NA,NA,1),
  col=c("blue","red","black","purple"),bg="white")

}
